{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала убедимся с помощью fastqc, что файлы получены из Illumina.\n",
    "\n",
    "Видим, что качество ридов сильно падает к концу, будем считать, что это так.\n",
    "\n",
    "GC-content так же выглядит адекватно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bwa_index] Pack FASTA... 0.00 sec\n",
      "[bwa_index] Construct BWT for the packed sequence...\n",
      "[bwa_index] 0.08 seconds elapse.\n",
      "[bwa_index] Update BWT... 0.00 sec\n",
      "[bwa_index] Pack forward-only FASTA... 0.00 sec\n",
      "[bwa_index] Construct SA from BWT and Occ... 0.03 sec\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa index ./MG1655-K12.first400K.fasta\n",
      "[main] Real time: 0.118 sec; CPU: 0.128 sec\n"
     ]
    }
   ],
   "source": [
    "! bwa index MG1655-K12.first400K.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bwa mem MG1655-K12.first400K.fasta ecoli_400K_err_1.fastq.gz ecoli_400K_err_2.fastq.gz > simple.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для анализа качества исправления ошибок будем пользоваться следующим подходом.\n",
    "\n",
    "Построим для базовых и исправленных выровненных ридов словари ошибок следующего вида:\n",
    "\n",
    "#### {<номер рида, forward/backward> : {позиция ошибки в референсе: <значение референса, значение рида>}}\n",
    "\n",
    "Сравнение исправленных ридов с неисправленными будем осуществлять с помощью таких словарей.\n",
    "\n",
    "P.S. Данный алгоритм подразумевает под удалениями только удаления целых ридов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(sam_f, fasta_f):\n",
    "    # Read reference\n",
    "    reference = []\n",
    "    for line in open(fasta_f):\n",
    "        if line.startswith('>'):\n",
    "            continue\n",
    "        reference.append(line.strip())\n",
    "    reference = list(''.join(reference))\n",
    "    reference_len = len(reference)\n",
    "    total_errors = {}\n",
    "    good_positions_n = 0\n",
    "    for num, line in enumerate(open(sam_f)):\n",
    "        if num and (num % 1e6 == 0):\n",
    "            print(f\"Processed {num} SAM-lines\")\n",
    "        if line.startswith('@'):\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if parts[2] == '*' or parts[5] == '*':\n",
    "            continue\n",
    "        start_pos = int(parts[3])\n",
    "        if start_pos == 0:\n",
    "            continue\n",
    "\n",
    "        read_name = parts[0]\n",
    "        read_type = 'f' if int(parts[8]) > 0 else 'b'\n",
    "        errors = {}\n",
    "        read = list(parts[9])\n",
    "        read_len = len(read)\n",
    "        end_pos = min(start_pos + read_len - 1, reference_len - 1)\n",
    "        reference_region = reference[start_pos-1:end_pos]\n",
    "        for pos, r in enumerate(reference_region):\n",
    "            if r != read[pos]: # and read[pos] != 'N':\n",
    "                errors[start_pos + pos] = (r, read[pos])\n",
    "            else:\n",
    "                good_positions_n += 1\n",
    "        if len(errors) > 0:\n",
    "            total_errors[(read_name, read_type)] = errors\n",
    "    return total_errors, good_positions_n\n",
    "\n",
    "def get_reads_names(sam_f):\n",
    "    names = set()\n",
    "    for num, line in enumerate(open(sam_f)):\n",
    "        if num and (num % 1e6 == 0):\n",
    "            print(f\"Processed {num} SAM-lines\")\n",
    "        if line.startswith('@'):\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if parts[2] == '*' or parts[5] == '*':\n",
    "            continue\n",
    "        start_pos = int(parts[3])\n",
    "        if start_pos == 0:\n",
    "            continue\n",
    "        read_name = parts[0]\n",
    "        read_type = 'f' if int(parts[8]) > 0 else 'b'\n",
    "        names.add((read_name, read_type))\n",
    "    return names\n",
    "\n",
    "def get_deleted_reads(total_reads, sam_f):\n",
    "    deleted_reads = total_reads.copy()\n",
    "    for num, line in enumerate(open(sam_f)):\n",
    "        if num and (num % 1e6 == 0):\n",
    "            print(f\"Processed {num} SAM-lines\")\n",
    "        if line.startswith('@'):\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if parts[2] == '*' or parts[5] == '*':\n",
    "            continue\n",
    "        start_pos = int(parts[3])\n",
    "        if start_pos == 0:\n",
    "            continue\n",
    "        read_name = parts[0]\n",
    "        read_type = 'f' if int(parts[8]) > 0 else 'b'\n",
    "        if (read_name, read_type) in total_reads:\n",
    "            deleted_reads.remove((read_name, read_type))\n",
    "    return deleted_reads\n",
    "\n",
    "def deleted_good_positions(errors, deleted_reads, sam_f):\n",
    "    n = 0\n",
    "    for num, line in enumerate(open(sam_f)):\n",
    "        if num and (num % 1e6 == 0):\n",
    "            print(f\"Processed {num} SAM-lines\")\n",
    "        if line.startswith('@'):\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if parts[2] == '*' or parts[5] == '*':\n",
    "            continue\n",
    "        start_pos = int(parts[3])\n",
    "        if start_pos == 0:\n",
    "            continue\n",
    "        read_name = parts[0]\n",
    "        read_type = 'f' if int(parts[8]) > 0 else 'b'\n",
    "        if (read_name, read_type) not in deleted_reads:\n",
    "            continue\n",
    "        read = list(parts[9])\n",
    "        read_len = len(read)\n",
    "        if (read_name, read_type) not in errors:\n",
    "            n += read_len\n",
    "            continue\n",
    "        errors_n = len(errors[(read_name, read_type)])\n",
    "        n += (read_len - errors_n)\n",
    "    return n\n",
    "\n",
    "def get_stat(first_errors, second_errors, first_sam, second_sam, first_good_n, second_good_n):\n",
    "    print('Get reads names from first file...')\n",
    "    first_reads = get_reads_names(first_sam)\n",
    "    print('Completed.')\n",
    "    print('Get deleted reads...')\n",
    "    second_deleted_reads = get_deleted_reads(first_reads, second_sam)\n",
    "    print('Completed.')    \n",
    "    \n",
    "    total_reads = len(first_reads)\n",
    "    deleted_reads = len(second_deleted_reads)\n",
    "    total_errors = 0\n",
    "    corrected_errors = 0\n",
    "    deleted_errors = 0\n",
    "    \n",
    "    print('Start comparing errors (1 -> 2)...')\n",
    "    for (read, read_type), positions in first_errors.items():\n",
    "        n = len(positions)\n",
    "        total_errors += n\n",
    "        if (read, read_type) in second_deleted_reads:\n",
    "            deleted_errors += n\n",
    "            continue\n",
    "        if (read, read_type) not in second_errors:\n",
    "            corrected_errors += n\n",
    "            continue\n",
    "        second_positions = second_errors[(read, read_type)]\n",
    "        for pos in positions.keys():\n",
    "            if pos not in second_positions:\n",
    "                corrected_errors += 1\n",
    "    print('Completed.')\n",
    "    print('Start comparing errors (2 -> 1)...')\n",
    "    new_errors = 0\n",
    "    for (read, read_type), positions in second_errors.items():\n",
    "        n = len(positions)\n",
    "        if (read, read_type) not in first_errors:\n",
    "            new_errors += n\n",
    "            continue\n",
    "        first_positions = first_errors[(read, read_type)]\n",
    "        for pos in positions.keys():\n",
    "            if pos not in first_positions:\n",
    "                new_errors += 1\n",
    "    print('Completed.')                \n",
    "    print('Start calculating deleted good bases...')\n",
    "    deleted_good_n = deleted_good_positions(first_errors, second_deleted_reads, first_sam)\n",
    "    print('Completed.')\n",
    "    \n",
    "    print('----')\n",
    "    print(f'Total reads: {total_reads}')\n",
    "    print(f'Deleted reads: {deleted_reads}({np.round(deleted_reads/total_reads, 4)*100}%)')\n",
    "    print(f'Total errors: {total_errors}')\n",
    "    print(f'Deleted errors: {deleted_errors}({np.round(deleted_errors/total_errors, 4)*100}%)')\n",
    "    print(f'Corrected errors: {corrected_errors}({np.round(corrected_errors/total_errors, 4)*100}%)')\n",
    "    print(f'New errors: {new_errors}({np.round(new_errors/total_errors, 4)*100}%)')\n",
    "    print(f'Profit: {np.round((1 - (total_errors - deleted_errors - corrected_errors + new_errors)/total_errors), 5)*100}% less errors')\n",
    "    \n",
    "    print('----')\n",
    "    print(f'Undetected error: {np.round((total_errors - deleted_errors - corrected_errors)/total_errors, 4)*100}%')\n",
    "    print(f'Detected & corrected error: {np.round(corrected_errors/total_errors, 4)*100}%')\n",
    "    print(f'Detected & removed error: {np.round(deleted_errors/total_errors, 4)*100}%')\n",
    "    print(f'Falsely corrected error: {np.round(new_errors/first_good_n, 4)*100}%')\n",
    "    print(f'Correctly unmodified base: {np.round((first_good_n - deleted_good_n - new_errors)/first_good_n, 4)*100}%')\n",
    "    print(f'Incorrectly removed base: {np.round(deleted_good_n/first_good_n, 4)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n"
     ]
    }
   ],
   "source": [
    "simple_errors, simple_good_positions_n = get_errors('simple.sam', 'MG1655-K12.first400K.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimmomatic\n",
    "\n",
    "Возьмем стандартные настройки из мануала + будем использовать далее только paired-риды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrimmomaticPE: Started with arguments:\n",
      " ecoli_400K_err_1.fastq.gz ecoli_400K_err_2.fastq.gz trimm_1_paired.fq.gz trimm_1_unpaired.fq.gz trimm_2_paired.fq.gz trimm_1_unpaired.fq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n",
      "Multiple cores found: Using 4 threads\n",
      "Quality encoding detected as phred33\n",
      "Input Read Pairs: 1381602 Both Surviving: 1309635 (94.79%) Forward Only Surviving: 38225 (2.77%) Reverse Only Surviving: 28561 (2.07%) Dropped: 5181 (0.37%)\n",
      "TrimmomaticPE: Completed successfully\n"
     ]
    }
   ],
   "source": [
    "! java -jar Trimmomatic-0.39/trimmomatic-0.39.jar PE \\\n",
    "ecoli_400K_err_1.fastq.gz ecoli_400K_err_2.fastq.gz \\\n",
    "trimm_1_paired.fq.gz trimm_1_unpaired.fq.gz \\\n",
    "trimm_2_paired.fq.gz trimm_1_unpaired.fq.gz \\\n",
    "LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bwa mem MG1655-K12.first400K.fasta trimm_1_paired.fq.gz trimm_2_paired.fq.gz > trimm.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n"
     ]
    }
   ],
   "source": [
    "trimm_errors, trimm_good_positions_n = get_errors('trimm.sam', 'MG1655-K12.first400K.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get reads names from first file...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "Get deleted reads...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "Start comparing errors (1 -> 2)...\n",
      "Completed.\n",
      "Start comparing errors (2 -> 1)...\n",
      "Completed.\n",
      "Start calculating deleted good bases...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "----\n",
      "Total reads: 2762617\n",
      "Deleted reads: 143396(5.19%)\n",
      "Total errors: 15105095\n",
      "Deleted errors: 2785389(18.44%)\n",
      "Corrected errors: 11017705(72.94%)\n",
      "New errors: 210222(1.39%)\n",
      "Profit: 89.98899999999999% less errors\n",
      "----\n",
      "Undetected error: 8.62%\n",
      "Detected & corrected error: 72.94%\n",
      "Detected & removed error: 18.44%\n",
      "Falsely corrected error: 0.08%\n",
      "Correctly unmodified base: 95.5%\n",
      "Incorrectly removed base: 4.42%\n"
     ]
    }
   ],
   "source": [
    "get_stat(simple_errors, trimm_errors, 'simple.sam', 'trimm.sam', simple_good_positions_n, trimm_good_positions_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае, так как Trimmomatic не исправляет ошибки, а только удаляет риды целиком или обрезает их, то все исправления на самом деле относятся к удалениям. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td style=\"width: 100px\"><b>Error in corrected reads</b></td>\n",
    "        <td style=\"width: 100px\"><b>Correct base in corrected reads</b></td>\n",
    "        <td style=\"width: 100px\"><b>Base is absent in corrected reads</b></td>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "        <td style=\"width: 100px\"><b>Error in raw data</b></td>\n",
    "        <td style=\"width: 100px\">8.62% Undetected error (FN)</td>\n",
    "        <td style=\"width: 100px\">0% Detected & corrected error (TP)</td>\n",
    "        <td style=\"width: 100px\">91.38% Detected and removed error (TP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"width: 100px\"><b>Correct base in raw data</b></td>\n",
    "        <td style=\"width: 100px\">0% Falsely corrected error (FP)</td>\n",
    "        <td style=\"width: 100px\">95.5% Correctly unmodified base (TN)</td>\n",
    "        <td style=\"width: 100px\">4.5% Incorrectly removed base (FP)</td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./SPAdes-3.14.1-Darwin/bin/spades.py -1 ecoli_400K_err_1.fastq.gz -2 ecoli_400K_err_2.fastq.gz -o spades_res --only-error-correction --threads 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! bwa mem MG1655-K12.first400K.fasta spades_res/corrected/ecoli_400K_err_1.fastq.00.0_0.cor.fastq.gz \\\n",
    "spades_res/corrected/ecoli_400K_err_2.fastq.00.0_0.cor.fastq.gz > spades.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n"
     ]
    }
   ],
   "source": [
    "spades_errors, spades_good_positions_n = get_errors('spades.sam', 'MG1655-K12.first400K.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get reads names from first file...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "Get deleted reads...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "Start comparing errors (1 -> 2)...\n",
      "Completed.\n",
      "Start comparing errors (2 -> 1)...\n",
      "Completed.\n",
      "Start calculating deleted good bases...\n",
      "Processed 1000000 SAM-lines\n",
      "Processed 2000000 SAM-lines\n",
      "Completed.\n",
      "----\n",
      "Total reads: 2762617\n",
      "Deleted reads: 46681(1.69%)\n",
      "Total errors: 15105095\n",
      "Deleted errors: 1079024(7.140000000000001%)\n",
      "Corrected errors: 4190820(27.74%)\n",
      "New errors: 37651757(249.27%)\n",
      "Profit: -214.377% less errors\n",
      "----\n",
      "Undetected error: 65.11%\n",
      "Detected & corrected error: 27.74%\n",
      "Detected & removed error: 7.140000000000001%\n",
      "Falsely corrected error: 14.42%\n",
      "Correctly unmodified base: 84.21%\n",
      "Incorrectly removed base: 1.37%\n"
     ]
    }
   ],
   "source": [
    "get_stat(simple_errors, spades_errors, 'simple.sam', 'spades.sam',\n",
    "         simple_good_positions_n, spades_good_positions_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь видим, что Spades исправил много верных оснований. Возможно, его необходимо аккуратнее тюнить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td style=\"width: 100px\"><b>Error in corrected reads</b></td>\n",
    "        <td style=\"width: 100px\"><b>Correct base in corrected reads</b></td>\n",
    "        <td style=\"width: 100px\"><b>Base is absent in corrected reads</b></td>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "        <td style=\"width: 100px\"><b>Error in raw data</b></td>\n",
    "        <td style=\"width: 100px\">65.11% Undetected error (FN)</td>\n",
    "        <td style=\"width: 100px\">27.74% Detected & corrected error (TP)</td>\n",
    "        <td style=\"width: 100px\">7.14% Detected and removed error (TP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"width: 100px\"><b>Correct base in raw data</b></td>\n",
    "        <td style=\"width: 100px\">14.42% Falsely corrected error (FP)</td>\n",
    "        <td style=\"width: 100px\">84.21% Correctly unmodified base (TN)</td>\n",
    "        <td style=\"width: 100px\">1.37% Incorrectly removed base (FP)</td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
